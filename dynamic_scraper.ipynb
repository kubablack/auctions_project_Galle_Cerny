{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic scraper\n",
    "#### Goal\n",
    "As the title suggests, this jupyter notebook includes a data scraper. The goal is to crawl through https://www.portaldrazeb.cz and to collect actual data about auctions, auctioneers and a list of auction attributes, which we will subsequently use to filter the auctions when the data is processed.  \n",
    "#### Problem\n",
    "The problem is that the webpage has dynamic content and therefore it is not possible to easily extract the data we need since the \"static\" source code differs from the \"dynamic\" one. The website also does not provide API (it actually does, however, not for us and not for the purposes we need). \n",
    "\n",
    "#### Solution\n",
    "We need to use proper methods to handle the dynamic content - our solution is the installation of package selenium and setting up a Google Chrome webdriver. We basically open a webpage and then we collect its source code, navigate between pages or change properties of the displayed content. Thanks to this package (and the webdriver which is also included in the GitHub repository) we manage to download the data we need. More detailed description of particular methods can be found in the class docstring and in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_auctions='https://www.portaldrazeb.cz/drazby/pripravovane' \n",
    "url_auctioneers = 'https://www.portaldrazeb.cz/drazebnici'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDownloader:\n",
    "    '''\n",
    "    This class crawls through dynamic content of https://www.portaldrazeb.cz and collects following things:\n",
    "    \n",
    "            1) soup object for every auctioneer\n",
    "            2) link to every auction + auction category (since the category is not within the auction page itself)\n",
    "            3) list of all possible values from drop-down menu (auction categories, regions and districts)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # we initiate the lists for data within particular methods\n",
    "        print('Downloader successfully initialized!')\n",
    "        print(' ')\n",
    "        print(DataDownloader.__doc__)\n",
    "        \n",
    "    def get_soups_of_auctioneers(self,link):\n",
    "        '''\n",
    "        Crawls through all pages of auctioneers and creates a soup object of everyone that is listed there right now.\n",
    "        '''\n",
    "        self.auctioneers_soups = []\n",
    "        # initiating a webdriver\n",
    "        driver = webdriver.Chrome('./chromedriver') \n",
    "        \n",
    "        # opening the link in Chrome\n",
    "        driver.get(link)  \n",
    "        time.sleep(5) \n",
    "        \n",
    "        # creating a soup object from the page source code\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")  \n",
    "        \n",
    "        # getting number of pages\n",
    "        last_page = int(soup.find('div',{'class':'el-pagination'}).findAll('li',{'class':'number'})[-1].text) \n",
    "        \n",
    "        # locating the element into which we write page number\n",
    "        page_number = driver.find_element_by_css_selector('input[type=\"number\"]')\n",
    "        \n",
    "        # looping through all pages and save the soups\n",
    "        for page in tqdm(range(1,last_page+1)):\n",
    "            # getting to a page (delete content, send number of the page, press Enter)\n",
    "            page_number.send_keys(Keys.BACK_SPACE)\n",
    "            page_number.send_keys(Keys.BACK_SPACE)\n",
    "            page_number.send_keys(str(page)) \n",
    "            page_number.send_keys(Keys.RETURN)\n",
    "            time.sleep(5)  \n",
    "\n",
    "            # save soups of particular auctioneers\n",
    "            html = driver.page_source   \n",
    "            soup = BeautifulSoup(html, \"html\") # soup of current page  \n",
    "            for i in soup.findAll('article'):\n",
    "                self.auctioneers_soups.append(i) # extract all auctioneers\n",
    "                \n",
    "        # close the window and check soups\n",
    "        driver.close()\n",
    "        if len(self.auctioneers_soups)>50:\n",
    "                print(f'Soup objects of auctioneers successfully downloaded! There are {len(self.auctioneers_soups)} of them right now.')\n",
    "                \n",
    "    def get_auction_links_and_categories(self,link):\n",
    "        '''\n",
    "        Crawls through all pages of auctions and from their source codes then collects link and category of every auction.\n",
    "        \n",
    "        '''\n",
    "        self.auction_links_and_categories = []\n",
    "        \n",
    "        # initiating the webdriver and opening the link\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        driver.get(url_auctions) \n",
    "        time.sleep(5) \n",
    "\n",
    "        # we need to click to the page to make the following step to work (the desired content is not available by now)\n",
    "        driver.find_element_by_css_selector('div[class=\"el-pagination\"]').click() \n",
    "\n",
    "        # we want to display 100 auctions/page (not the default 20) to decrease the scraping time  \n",
    "        el = driver.find_element_by_css_selector('div[class=\"el-pagination\"]').find_element_by_css_selector('i[class=\"el-select__caret el-input__icon el-icon-arrow-up\"]')\n",
    "        time.sleep(5)\n",
    "        el.click()\n",
    "        time.sleep(5)\n",
    "        new_el = el.find_element_by_xpath('//*[ text() = \"100/page\" ]')\n",
    "        new_el.click()\n",
    "        time.sleep(15)\n",
    "        \n",
    "        # getting number of pages from a soup\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html\")\n",
    "        last_page = int(soup.find('div',{'class':'el-pagination'}).findAll('li',{'class':'number'})[-1].text) # get number of pages\n",
    "        \n",
    "        # getting source codes from all pages and saving as soups\n",
    "        page_number = driver.find_element_by_css_selector('input[type=\"number\"]') # locating element into which we write page number\n",
    "        auctions_pages_soups = []\n",
    "        for page in tqdm(range(1,last_page+1)):\n",
    "            # get to a page\n",
    "            page_number.send_keys(Keys.BACK_SPACE)\n",
    "            page_number.send_keys(Keys.BACK_SPACE)\n",
    "            page_number.send_keys(str(page))\n",
    "            page_number.send_keys(Keys.RETURN)\n",
    "            time.sleep(15)\n",
    "            # save soup object of the page\n",
    "            html = driver.page_source \n",
    "            auctions_pages_soups.append(BeautifulSoup(html, \"html\"))\n",
    "\n",
    "        for soup in auctions_pages_soups:\n",
    "            for i in soup.findAll('article'):\n",
    "                # extracting link\n",
    "                auction = []\n",
    "                auction.append(i.find('a')['href'])\n",
    "\n",
    "                # extracting categories\n",
    "                categ = soup.find('article').find('tbody').findAll('tr')[1].find('span').text.lstrip('/').split('/')\n",
    "                auction.append(categ)\n",
    "                self.auction_links_and_categories.append(auction) # saving the data\n",
    "        \n",
    "        # closing the window and checking whether something downloaded\n",
    "        driver.close()\n",
    "        if len(self.auction_links_and_categories)>200:\n",
    "            print(f'Auction links and categories successfully downloaded! There are {len(self.auction_links_and_categories)} auctions right now.')\n",
    "    def get_items_from_dropdown_menu(self,link):\n",
    "        '''\n",
    "        Downloads all auctions categories, regions and districts.\n",
    "        '''\n",
    "        self.categories = []\n",
    "        self.regions_and_districts = []\n",
    "        \n",
    "        # initiating the webdriver and opening the link\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        driver.get(url_auctions) \n",
    "        time.sleep(5) \n",
    "        \n",
    "        # saving source code, extracting auction categories\n",
    "        html = driver.page_source \n",
    "        soup = BeautifulSoup(html, \"html\")\n",
    "        for categ in soup.findAll('ul',{'class':'el-scrollbar__view el-select-dropdown__list'})[0].findAll('span'):\n",
    "            self.categories.append(categ.text)\n",
    "            \n",
    "        # extracting regions and districts\n",
    "        for region in soup.findAll('ul',{'class':'el-scrollbar__view el-select-dropdown__list'})[1].findAll('ul',{'class':'el-select-group__wrap'}):\n",
    "            aux = []\n",
    "            for district in region.findAll('span'):\n",
    "                aux.append(district.text)\n",
    "            self.regions_and_districts.append([region.find('li').text,aux])\n",
    "        \n",
    "        # closing the window and checking whether everyhing downloaded\n",
    "        driver.close()\n",
    "        if (len(self.categories) > 5) & (len(self.regions_and_districts) == 14):\n",
    "            print('Auction categories, regions and districts successfully downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloader successfully initialized!\n",
      " \n",
      "\n",
      "    This class crawls through dynamic content of https://www.portaldrazeb.cz and collects following things:\n",
      "    \n",
      "            1) soup object for every auctioneer\n",
      "            2) link to every auction + auction category (since the category is not within the auction page itself)\n",
      "            3) list of all possible values from drop-down menu (auction categories, regions and districts)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "down = DataDownloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:46<00:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup objects of auctioneers successfully downloaded! There are 157 of them right now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "down.get_soups_of_auctioneers('https://www.portaldrazeb.cz/drazebnici') # takes approx. 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [02:56<00:00, 16.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auction links and categories successfully downloaded! There are 1058 auctions right now.\n"
     ]
    }
   ],
   "source": [
    "down.get_auction_links_and_categories('https://www.portaldrazeb.cz/drazby/pripravovane') # takes approx. 4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auction categories, regions and districts successfully downloaded!\n"
     ]
    }
   ],
   "source": [
    "down.get_items_from_dropdown_menu('https://www.portaldrazeb.cz/drazby/pripravovane')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
