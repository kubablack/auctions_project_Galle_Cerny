{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_auctions='https://www.portaldrazeb.cz/drazby/pripravovane'  # page we start from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the webdriver\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "# open the link in Chrome\n",
    "driver.get(url_auctions) \n",
    "\n",
    "# make sure page loads\n",
    "time.sleep(5) \n",
    "\n",
    "# we need to click to the page to make the following step to work\n",
    "driver.find_element_by_css_selector('div[class=\"el-pagination\"]').click() \n",
    "\n",
    "# we want to display 100 auctions/page (not the default 20) to decrease scraping time  \n",
    "el = driver.find_element_by_css_selector('div[class=\"el-pagination\"]').find_element_by_css_selector('i[class=\"el-select__caret el-input__icon el-icon-arrow-up\"]')\n",
    "time.sleep(5)\n",
    "el.click()\n",
    "time.sleep(5)\n",
    "new_el = el.find_element_by_xpath('//*[ text() = \"100/page\" ]')\n",
    "new_el.click()\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of pages\n",
    "last_page = int(soup.find('div',{'class':'el-pagination'}).findAll('li',{'class':'number'})[-1].text) # get number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctions_pages_soups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [02:54<00:00, 15.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# getting source codes from all pages and saving as soups\n",
    "page_number = driver.find_element_by_css_selector('input[type=\"number\"]') # locating element into which we write page number\n",
    "for page in tqdm(range(1,last_page+1)):\n",
    "    # get to a page\n",
    "    page_number.send_keys(Keys.BACK_SPACE)\n",
    "    page_number.send_keys(Keys.BACK_SPACE)\n",
    "    page_number.send_keys(str(page))\n",
    "    page_number.send_keys(Keys.RETURN)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    # save soup object of the page\n",
    "    html = driver.page_source \n",
    "    auctions_pages_soups.append(BeautifulSoup(html, \"html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctions_link_and_category = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for soup in auctions_pages_soups:\n",
    "    for i in soup.findAll('article'):\n",
    "        # extract link\n",
    "        auction = []\n",
    "        auction.append(i.find('a')['href'])\n",
    "        \n",
    "        # extract categories\n",
    "        categ = soup.find('article').find('tbody').findAll('tr')[1].find('span').text.lstrip('/').split('/')\n",
    "        auction.append(categ)\n",
    "        auctions_link_and_category.append(auction) # save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://www.portaldrazeb.cz/drazba/054ex633-08-274-dlxd5',\n",
       "  ['Nemovitosti', 'Pozemek']],\n",
       " ['https://www.portaldrazeb.cz/drazba/054ex578-05-302-1ld2g',\n",
       "  ['Nemovitosti', 'Pozemek']],\n",
       " ['https://www.portaldrazeb.cz/drazba/054ex550-04-212-73wpo',\n",
       "  ['Nemovitosti', 'Pozemek']],\n",
       " ['https://www.portaldrazeb.cz/drazba/054ex233-05-313-w8p9z',\n",
       "  ['Nemovitosti', 'Pozemek']],\n",
       " ['https://www.portaldrazeb.cz/drazba/169ex7525-11-164-95llg',\n",
       "  ['Nemovitosti', 'Pozemek']]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auctions_link_and_category[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_auctioneers = 'https://www.portaldrazeb.cz/drazebnici'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('./chromedriver') \n",
    "driver.get(url_auctioneers)  # open the link in Chrome\n",
    "time.sleep(5)  # make sure page loads\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")  # create a soup object from the page source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "auctioneers_soups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page = int(soup.find('div',{'class':'el-pagination'}).findAll('li',{'class':'number'})[-1].text) # get number of pages\n",
    "page_number = driver.find_element_by_css_selector('input[type=\"number\"]') # locating the element into which we write page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:37<00:00,  5.30s/it]\n"
     ]
    }
   ],
   "source": [
    "for page in tqdm(range(1,last_page+1)):\n",
    "    # get to a page\n",
    "    page_number.send_keys(Keys.BACK_SPACE)\n",
    "    page_number.send_keys(Keys.BACK_SPACE)\n",
    "    page_number.send_keys(str(page))\n",
    "    page_number.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # make sure page loads\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    # save soups of particular auctioneers\n",
    "    html = driver.page_source   \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # soup of current page  \n",
    "    for i in soup.findAll('article'):\n",
    "        auctioneers_soups.append(i) # extract all auctioneers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDownloader:\n",
    "    '''\n",
    "    This class crawls through dynamic content of https://www.portaldrazeb.cz and collects following things:\n",
    "            1) soup object for every auctioneer\n",
    "            2) link to every auction + category of the auction (since the category is not within the auction page itself)\n",
    "            3) list of all possible categories and regions of auctions (from drop-down menu)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.auctioneers_soups = []\n",
    "        self.auctions_link_and_category = []\n",
    "        self.categories = []\n",
    "        self.regions = []\n",
    "        print('''Downloader successfully initialized!''')\n",
    "    def get_soups_of_auctioneers(self,link):\n",
    "        # initiating a webdriver\n",
    "        driver = webdriver.Chrome('./chromedriver') \n",
    "        \n",
    "        # opening the link in Chrome\n",
    "        driver.get(link)  \n",
    "        \n",
    "        # making sure page loads\n",
    "        time.sleep(5) \n",
    "        \n",
    "        # creating a soup object from the page source code\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")  \n",
    "        \n",
    "        # getting number of pages\n",
    "        last_page = int(soup.find('div',{'class':'el-pagination'}).findAll('li',{'class':'number'})[-1].text) \n",
    "        \n",
    "        # locating the element into which we write page number\n",
    "        page_number = driver.find_element_by_css_selector('input[type=\"number\"]')\n",
    "        \n",
    "        # loop through all pages and save the soups\n",
    "        for page in tqdm(range(1,last_page+1)):\n",
    "            # get to a page\n",
    "            page_number.send_keys(Keys.BACK_SPACE)\n",
    "            page_number.send_keys(Keys.BACK_SPACE)\n",
    "            page_number.send_keys(str(page))\n",
    "            page_number.send_keys(Keys.RETURN)\n",
    "\n",
    "            # make sure page loads\n",
    "            time.sleep(5)  \n",
    "\n",
    "            # save soups of particular auctioneers\n",
    "            html = driver.page_source   \n",
    "            soup = BeautifulSoup(html, \"html.parser\") # soup of current page  \n",
    "            for i in soup.findAll('article'):\n",
    "                self.auctioneers_soups.append(i) # extract all auctioneers\n",
    "                \n",
    "        # close the window and check soups\n",
    "        driver.close()\n",
    "        if len(self.auctioneers_soups)>50:\n",
    "                print('Soup objects of auctioneers successfully downloaded!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloader successfully initialized!\n"
     ]
    }
   ],
   "source": [
    "down = DataDownloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:43<00:00,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup objects of auctioneers successfully downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "down.get_soups_of_auctioneers('https://www.portaldrazeb.cz/drazebnici')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(down.auctioneers_soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
